{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QCustom model Training using Hyderabad Dataset DS3\n",
    "Date: 2024-07-15\n",
    "\n",
    "Author: Ziad Tamim\n",
    "\n",
    "Discription:\n",
    "Thsi Script includes the training of the QCustom model. This includes model structure, Quantisation and knowledge disilation.  \n",
    "\n",
    "Inputs:\n",
    "* Dataset\n",
    "\n",
    "Outputs:\n",
    "* Custom model (without quanisation)\n",
    "* QCustom model (quanized Custom model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to data\n",
    "occupied = 'C:/Users/ziadt/Desktop/Projects/MSc Project implimintation/Datasets/Parkingdata/Parkingdata/Occupied'\n",
    "empty = 'C:/Users/ziadt/Desktop/Projects/MSc Project implimintation/Datasets/Parkingdata/Parkingdata/Empty'\n",
    "\n",
    "# load images\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def load_images(path):\n",
    "    images = []\n",
    "    for filename in os.listdir(path):\n",
    "        img = cv2.imread(os.path.join(path,filename))\n",
    "        if img is not None:\n",
    "            images.append(img)\n",
    "    return images\n",
    "\n",
    "occupied_images = load_images(occupied)\n",
    "empty_images = load_images(empty)\n",
    "\n",
    "# check the number of images\n",
    "print('Occupied images: %d' % len(occupied_images))\n",
    "print('Empty images: %d' % len(empty_images))\n",
    "\n",
    "# calculate the avrge size of the images\n",
    "def avg_size(images):\n",
    "    sizes = [img.shape[:2] for img in images]\n",
    "    return np.mean(sizes, axis=0).astype(int)\n",
    "\n",
    "occupied_avg_size = avg_size(occupied_images)\n",
    "empty_avg_size = avg_size(empty_images)\n",
    "\n",
    "print('Occupied average size: %s' % str(occupied_avg_size))\n",
    "print('Empty average size: %s' % str(empty_avg_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check image and label\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "def show_image(images, title):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i in range(9):\n",
    "        plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(cv2.cvtColor(random.choice(images), cv2.COLOR_BGR2RGB))\n",
    "        plt.axis('off')\n",
    "    plt.suptitle(title)\n",
    "    plt.show()\n",
    "\n",
    "show_image(occupied_images, 'Occupied')\n",
    "show_image(empty_images, 'Empty')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess images\n",
    "def preprocess_images(images):\n",
    "    processed_images = []\n",
    "    for img in images:\n",
    "        # Convert BGR to RGB before resizing\n",
    "        # img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        # resize image\n",
    "        img = cv2.resize(img, (150, 150))\n",
    "        # normalize image\n",
    "        img = img.astype(np.float32) / 255.0\n",
    "        processed_images.append(img)\n",
    "    return processed_images\n",
    "\n",
    "occupied_images = preprocess_images(occupied_images)\n",
    "empty_images = preprocess_images(empty_images)\n",
    "\n",
    "show_image(occupied_images, 'Occupied')\n",
    "show_image(empty_images, 'Empty')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming occupied_images_processed and empty_images_processed are lists of numpy arrays\n",
    "# Label the data\n",
    "occupied_labels = [1] * len(occupied_images)\n",
    "empty_labels = [0] * len(empty_images)\n",
    "\n",
    "# Combine the data\n",
    "X = np.array(occupied_images + empty_images)\n",
    "y = np.array(occupied_labels + empty_labels)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Now X_train, X_test, y_train, and y_test are available for training and testing\n",
    "print(\"Training data shape:\", X_train.shape)\n",
    "print(\"Test data shape:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Custom model structure (Student model)\n",
    "### Discription\n",
    "This code defines the Custom model (Student) use in a knowledge distillation process. The model is intended to be trained by learning from a pre-trained \"teacher\" model. The student model takes 150x150x3 input images and passes them through a series of layers, including initial convolutional layers, multiple MobileNet blocks, and a global average pooling layer. The final output layer uses a sigmoid activation function to produce a single probability score for binary classification (Occupied or Free). The model's compact architecture makes it suitable for distillation, where it will be trained to mimic the behavior of a more complex teacher model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, DepthwiseConv2D, Conv2D, BatchNormalization\n",
    "from tensorflow.keras.layers import ReLU, GlobalAveragePooling2D, Dense\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "\n",
    "# MobileNet block\n",
    "def mobilnet_block(x, filters, strides): # this function is used to create a mobilenet block consisting of depthwise convolution followed by pointwise convolution\n",
    "    x = DepthwiseConv2D(kernel_size=3, strides=strides, padding='same')(x) # depthwise convolution\n",
    "    x = BatchNormalization()(x) \n",
    "    x = ReLU()(x)\n",
    "    \n",
    "    x = Conv2D(filters=filters, kernel_size=1, strides=1)(x) # pointwise convolution\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    \n",
    "    return x # return the output of the block\n",
    "\n",
    "# Input specification for the model\n",
    "input = Input(shape=(150, 150, 3))  # image input size is 150x150x3\n",
    "x = Conv2D(filters=16, kernel_size=3, strides=2, padding='same')(input) # first layer conv2d\n",
    "x = BatchNormalization()(x) # first layer batch normalization\n",
    "x = ReLU()(x) # first layer ReLU\n",
    "\n",
    "# Main part of the model\n",
    "x = mobilnet_block(x, filters=5, strides=1) # second layer mobilenet block\n",
    "x = mobilnet_block(x, filters=5, strides=2) # third layer mobilenet block\n",
    "x = mobilnet_block(x, filters=12, strides=1) # fourth layer mobilenet block\n",
    "x = mobilnet_block(x, filters=12, strides=2) # fifth layer mobilenet block\n",
    "x = mobilnet_block(x, filters=24, strides=1) # sixth layer mobilenet block\n",
    "x = mobilnet_block(x, filters=24, strides=2) # seventh layer mobilenet block\n",
    "x = mobilnet_block(x, filters=24, strides=2) # eighth layer mobilenet block\n",
    "\n",
    "# Adjusting for binary classification\n",
    "x = GlobalAveragePooling2D()(x)  # Changed from AvgPool2D to GlobalAveragePooling2D\n",
    "output = Dense(units=1, activation='sigmoid')(x)  # Changed to one unit with sigmoid activation\n",
    "\n",
    "# Create the model\n",
    "student = Model(inputs=input, outputs=output) # create the model \n",
    "student.summary()\n",
    "student.input_shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resnet50 model (Teacher model)\n",
    "This code sets up the teacher model for knowledge distillation using a pre-trained ResNet50 architecture. The ResNet50 model's layers are frozen to retain the pre-trained ImageNet weights. A global average pooling layer and a sigmoid-activated dense layer are added on top to adapt the model for binary classification. This model will serve as the teacher in the knowledge distillation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train on resnet\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "\n",
    "# load the model\n",
    "resnet = ResNet50(include_top=False, weights='imagenet', input_shape=(150, 150, 3))\n",
    "resnet.summary()\n",
    "\n",
    "# Freeze the layers\n",
    "for layer in resnet.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add the top layers\n",
    "x = resnet.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1, activation='sigmoid')(x)\n",
    "teacher = Model(resnet.input, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distillation Model Setup\n",
    "This code defines the Distiller class, a custom TensorFlow model designed to facilitate knowledge distillation. The class combines a teacher and student model, allowing the student to learn from both the true labels and the teacher's predictions. The distillation process is controlled by parameters like alpha (balancing student and distillation loss) and temperature (smoothing the teacher's predictions). The Distiller is then instantiated, compiled with binary cross-entropy for the student loss, and Kullback-Leibler divergence for the distillation loss, and is ready for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Distiller(tf.keras.Model): # this class is used to create a distiller model that will be used to train the student model\n",
    "    def __init__(self, student, teacher):\n",
    "        super(Distiller, self).__init__()\n",
    "        self.teacher = teacher\n",
    "        self.student = student\n",
    "\n",
    "    def compile( # this function is used to compile the model\n",
    "        self,\n",
    "        optimizer,\n",
    "        metrics,\n",
    "        student_loss_fn,\n",
    "        distillation_loss_fn,\n",
    "        alpha=0.1, # alpha is the weight given to the student loss\n",
    "        temperature=3, # temperature is used to soften the predictions\n",
    "    ):\n",
    "        super(Distiller, self).compile(optimizer=optimizer, metrics=metrics) # compile the model with the optimizer and metrics\n",
    "        self.student_loss_fn = student_loss_fn\n",
    "        self.distillation_loss_fn = distillation_loss_fn\n",
    "        self.alpha = alpha\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def train_step(self, data): # this function is used to train the model using the distillation loss\n",
    "        x, y = data\n",
    "        teacher_predictions = self.teacher(x, training=False)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            student_predictions = self.student(x, training=True)\n",
    "            student_loss = self.student_loss_fn(y, student_predictions)\n",
    "            distillation_loss = self.distillation_loss_fn(\n",
    "                tf.nn.softmax(teacher_predictions / self.temperature, axis=1),\n",
    "                tf.nn.softmax(student_predictions / self.temperature, axis=1),\n",
    "            )\n",
    "            loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss # calculate the loss using the distillation loss\n",
    "\n",
    "        gradients = tape.gradient(loss, self.student.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.student.trainable_variables))\n",
    "        self.compiled_metrics.update_state(y, student_predictions)\n",
    "        results = {m.name: m.result() for m in self.metrics}\n",
    "        results.update({\"student_loss\": student_loss, \"distillation_loss\": distillation_loss}) # update the results with the student loss and distillation loss\n",
    "        return results\n",
    "\n",
    "    #Test 2\n",
    "    def test_step(self, data): # this function is used to test the model using the distillation loss\n",
    "        x, y = data\n",
    "        student_predictions = self(x, training=False)\n",
    "        student_loss = self.student_loss_fn(y, student_predictions)\n",
    "\n",
    "        # Update metrics\n",
    "        self.compiled_metrics.update_state(y, student_predictions)\n",
    "\n",
    "        # Collect metrics results\n",
    "        results = {m.name: m.result() for m in self.metrics}\n",
    "        results['loss'] = student_loss\n",
    "        return results\n",
    "\n",
    "    def call(self, inputs, training=False): # this function is used to call the model\n",
    "        if training:\n",
    "            return self.student(inputs, training=True)\n",
    "        else:\n",
    "            return self.student(inputs, training=False)\n",
    "\n",
    "# Create the distiller instance again and compile it\n",
    "distiller = Distiller(student=student, teacher=teacher)\n",
    "distiller.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    metrics=[tf.keras.metrics.BinaryAccuracy()],\n",
    "    student_loss_fn=tf.keras.losses.BinaryCrossentropy(),\n",
    "    distillation_loss_fn=tf.keras.losses.KLDivergence(),\n",
    "    alpha=0.1,\n",
    "    temperature=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard initialization\n",
    "from datetime import datetime\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "path_to_logs = \"C:/Users/ziadt/Desktop/Projects/MSc Project implimintation/Model Training/CustomeNEt/training supporting material/logs/fit/\"\n",
    "\n",
    "# Create a TensorBoard callback\n",
    "log_dir = path_to_logs + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the distiller\n",
    "distiller.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=1, callbacks=[tensorboard_callback])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy perfomance on the test set\n",
    "distiller.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Custom model without Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "student.save('C:/Users/ziadt/Desktop/Projects/MSc Project implimintation/Model Training/CustomeNEt/training supporting material/student_model_DS3.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "from tensorflow.keras.models import load_model\n",
    "model = load_model('C:/Users/ziadt/Desktop/Projects/MSc Project implimintation/Model Training/CustomeNEt/training supporting material/student_model_DS3.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post Training Interger Quantization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full integer quantization\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(distiller.student)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model to disk\n",
    "open('QCustom_D3_V1.tflite', 'wb').write(tflite_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference speed of the QCustom model on a single slot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Load the TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "\n",
    "# test data\n",
    "image = X_test[0]\n",
    "\n",
    "\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "image = cv2.resize(image, (150, 150))\n",
    "image = np.expand_dims(image, axis=0)\n",
    "image = image / 255.0\n",
    "image = tf.convert_to_tensor(image, dtype=tf.float32)\n",
    "\n",
    "\n",
    "# Test the model on random input data.\n",
    "times = []\n",
    "for i in range(100):\n",
    "    start_time = time.time()\n",
    "    interpreter.set_tensor(input_details[0]['index'], image)\n",
    "    interpreter.invoke()\n",
    "    end_time = time.time()\n",
    "    times.append(end_time - start_time)\n",
    "\n",
    "print('Average inference time for 100 images with quantized model:', np.mean(times))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newvm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
