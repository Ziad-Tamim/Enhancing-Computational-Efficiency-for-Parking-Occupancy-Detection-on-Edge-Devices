{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QCustom model Training using CNRPark-EXT DS1\n",
    "Date: 2024-07-15\n",
    "\n",
    "Author: Ziad Tamim\n",
    "\n",
    "Discription:\n",
    "Thsi Script includes the training of the QCustom model. This includes model structure, Quantisation  and knowledge disilation.  \n",
    "\n",
    "Inputs:\n",
    "* Dataset\n",
    "\n",
    "Outputs:\n",
    "* Custom model (with no quanisation)\n",
    "* QCustom model (quanized Custom model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to files\n",
    "traing_path = 'C:/Users/ziadt/Desktop/Projects/MSc Project implimintation/Datasets/CNP-EXT - Dataset/CNR-EXT-Patches-150x150/LABELS/train.txt'\n",
    "test_path = 'C:/Users/ziadt/Desktop/Projects/MSc Project implimintation/Datasets/CNP-EXT - Dataset/CNR-EXT-Patches-150x150/LABELS/train.txt'\n",
    "val_path = 'C:/Users/ziadt/Desktop/Projects/MSc Project implimintation/Datasets/CNP-EXT - Dataset/CNR-EXT-Patches-150x150/LABELS/train.txt'\n",
    "base_path = 'C:/Users/ziadt/Desktop/Projects/MSc Project implimintation/Datasets/CNP-EXT - Dataset/CNR-EXT-Patches-150x150/PATCHES'  # Ensure this is the correct base path to your images\n",
    "\n",
    "\n",
    "# Open files\n",
    "train_file = open(traing_path, 'r')\n",
    "test_file = open(test_path, 'r')\n",
    "val_file = open(val_path, 'r')\n",
    "\n",
    "# Read lines\n",
    "train_lines = train_file.readlines()\n",
    "test_lines = test_file.readlines()\n",
    "val_lines = val_file.readlines()\n",
    "train_file.close()\n",
    "test_file.close()\n",
    "val_file.close()\n",
    "\n",
    "# Lists to store images and labels\n",
    "train_images = []\n",
    "train_labels = []\n",
    "test_images = []\n",
    "test_labels = []\n",
    "val_images = []\n",
    "val_labels = []\n",
    "\n",
    "# Split lines\n",
    "for line in train_lines:\n",
    "    line = line.split(' ')\n",
    "    train_images.append(line[0])\n",
    "    train_labels.append(int(line[1]))\n",
    "\n",
    "for line in test_lines:\n",
    "    line = line.split(' ')\n",
    "    test_images.append(line[0])\n",
    "    test_labels.append(int(line[1]))\n",
    "\n",
    "for line in val_lines:\n",
    "    line = line.split(' ')\n",
    "    val_images.append(line[0])\n",
    "    val_labels.append(int(line[1]))\n",
    "\n",
    "# Print info\n",
    "print('Train images:', len(train_images))\n",
    "print('Train labels:', len(train_labels))\n",
    "print('Test images:', len(test_images))\n",
    "print('Test labels:', len(test_labels)) \n",
    "print('Val images:', len(val_images))\n",
    "print('Val labels:', len(val_labels))\n",
    "print('Total images:', len(train_images) + len(test_images) + len(val_images))\n",
    "print('\\n')\n",
    "print('train image: ', train_images[0])\n",
    "print('train label: ', train_labels[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View data for inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Assuming train_images[0] contains the relative path from the notebook's current directory\n",
    "n = 1\n",
    "image_path = train_images[n]\n",
    "full_image_path = os.path.join(base_path, image_path)\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(full_image_path):\n",
    "    image = cv2.imread(full_image_path)\n",
    "    print(full_image_path)\n",
    "    if image is not None:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        plt.imshow(image)\n",
    "        plt.show()\n",
    "        print('Label:', train_labels[n])\n",
    "    else:\n",
    "        print(\"Failed to load the image. The file may be corrupted.\")\n",
    "else:\n",
    "    print(\"Image path does not exist:\", full_image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# discription: This code is used to load the images and labels from the dataset and create a dataset object that can be used to train the model.\n",
    "# The dataset is then preprocessed for performance using the tf.data API. The images are resized to 150x150 pixels and the dataset is shuffled,\n",
    "# batched, and prefetched for performance. \n",
    "# The dataset is then ready to be used for training a model.\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "\n",
    "def parse_function(filename, label):\n",
    "    # Read an image from a file\n",
    "    image_string = tf.io.read_file(filename)\n",
    "    # Decode it into a dense vector\n",
    "    image_decoded = tf.image.decode_jpeg(image_string, channels=3)\n",
    "    # Resize it to fixed shape\n",
    "    image_resized = tf.image.resize(image_decoded, [150, 150]) # change size to 227x227 for AlexNet\n",
    "    # Ensure label is also returned correctly\n",
    "    return image_resized, label\n",
    "\n",
    "def create_dataset(filepaths, labels):\n",
    "    path_ds = tf.data.Dataset.from_tensor_slices(filepaths)\n",
    "    labels_ds = tf.data.Dataset.from_tensor_slices(labels)\n",
    "    # Use dataset map with both filename and label\n",
    "    full_dataset = tf.data.Dataset.zip((path_ds, labels_ds))\n",
    "    image_label_ds = full_dataset.map(parse_function, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    return image_label_ds\n",
    "\n",
    "# Full file paths for the images\n",
    "train_paths = [os.path.join(base_path, img) for img in train_images]\n",
    "test_paths = [os.path.join(base_path, img) for img in test_images]\n",
    "val_paths = [os.path.join(base_path, img) for img in val_images]\n",
    "\n",
    "print(train_paths[0])\n",
    "# Create datasets\n",
    "train_ds = create_dataset(train_paths, train_labels)\n",
    "test_ds = create_dataset(test_paths, test_labels)\n",
    "val_ds = create_dataset(val_paths, val_labels)\n",
    "\n",
    "# Configure for performance\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "train_ds = train_ds.shuffle(1000).batch(32).prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds = test_ds.batch(32).prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.batch(32).prefetch(buffer_size=AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Custom model structure (Student model)\n",
    "### Discription\n",
    "This code defines the Custom model (Student) use in a knowledge distillation process. The model is intended to be trained by learning from a pre-trained \"teacher\" model. The student model takes 150x150x3 input images and passes them through a series of layers, including initial convolutional layers, multiple MobileNet blocks, and a global average pooling layer. The final output layer uses a sigmoid activation function to produce a single probability score for binary classification (Occupied or Free). The model's compact architecture makes it suitable for distillation, where it will be trained to mimic the behavior of a more complex teacher model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, DepthwiseConv2D, Conv2D, BatchNormalization\n",
    "from tensorflow.keras.layers import ReLU, GlobalAveragePooling2D, Dense\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "\n",
    "# MobileNet block\n",
    "def mobilnet_block(x, filters, strides): # this function is used to create a mobilenet block consisting of depthwise convolution followed by pointwise convolution\n",
    "    x = DepthwiseConv2D(kernel_size=3, strides=strides, padding='same')(x) # depthwise convolution\n",
    "    x = BatchNormalization()(x) \n",
    "    x = ReLU()(x)\n",
    "    \n",
    "    x = Conv2D(filters=filters, kernel_size=1, strides=1)(x) # pointwise convolution\n",
    "    x = BatchNormalization()(x)\n",
    "    x = ReLU()(x)\n",
    "    \n",
    "    return x # return the output of the block\n",
    "\n",
    "# Input specification for the model\n",
    "input = Input(shape=(150, 150, 3))  # image input size is 150x150x3\n",
    "x = Conv2D(filters=16, kernel_size=3, strides=2, padding='same')(input) # first layer conv2d\n",
    "x = BatchNormalization()(x) # first layer batch normalization\n",
    "x = ReLU()(x) # first layer ReLU\n",
    "\n",
    "# Main part of the model\n",
    "x = mobilnet_block(x, filters=5, strides=1) # second layer mobilenet block\n",
    "x = mobilnet_block(x, filters=5, strides=2) # third layer mobilenet block\n",
    "x = mobilnet_block(x, filters=12, strides=1) # fourth layer mobilenet block\n",
    "x = mobilnet_block(x, filters=12, strides=2) # fifth layer mobilenet block\n",
    "x = mobilnet_block(x, filters=24, strides=1) # sixth layer mobilenet block\n",
    "x = mobilnet_block(x, filters=24, strides=2) # seventh layer mobilenet block\n",
    "x = mobilnet_block(x, filters=24, strides=2) # eighth layer mobilenet block\n",
    "\n",
    "# Adjusting for binary classification\n",
    "x = GlobalAveragePooling2D()(x)  # Changed from AvgPool2D to GlobalAveragePooling2D\n",
    "output = Dense(units=1, activation='sigmoid')(x)  # Changed to one unit with sigmoid activation\n",
    "\n",
    "# Create the model\n",
    "student = Model(inputs=input, outputs=output) # create the model \n",
    "student.summary()\n",
    "student.input_shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resnet50 model (Teacher model)\n",
    "This code sets up the teacher model for knowledge distillation using a pre-trained ResNet50 architecture. The ResNet50 model's layers are frozen to retain the pre-trained ImageNet weights. A global average pooling layer and a sigmoid-activated dense layer are added on top to adapt the model for binary classification. This model will serve as the teacher in the knowledge distillation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train on resnet\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "\n",
    "# load the model\n",
    "resnet = ResNet50(include_top=False, weights='imagenet', input_shape=(150, 150, 3))\n",
    "resnet.summary()\n",
    "\n",
    "# Freeze the layers\n",
    "for layer in resnet.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Add the top layers\n",
    "x = resnet.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1, activation='sigmoid')(x)\n",
    "teacher = Model(resnet.input, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distillation Model Setup\n",
    "This code defines the Distiller class, a custom TensorFlow model designed to facilitate knowledge distillation. The class combines a teacher and student model, allowing the student to learn from both the true labels and the teacher's predictions. The distillation process is controlled by parameters like alpha (balancing student and distillation loss) and temperature (smoothing the teacher's predictions). The Distiller is then instantiated, compiled with binary cross-entropy for the student loss, and Kullback-Leibler divergence for the distillation loss, and is ready for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Distiller(tf.keras.Model): # this class is used to create a distiller model that will be used to train the student model\n",
    "    def __init__(self, student, teacher):\n",
    "        super(Distiller, self).__init__()\n",
    "        self.teacher = teacher\n",
    "        self.student = student\n",
    "\n",
    "    def compile( # this function is used to compile the model\n",
    "        self,\n",
    "        optimizer,\n",
    "        metrics,\n",
    "        student_loss_fn,\n",
    "        distillation_loss_fn,\n",
    "        alpha=0.1, # alpha is the weight given to the student loss\n",
    "        temperature=3, # temperature is used to soften the predictions\n",
    "    ):\n",
    "        super(Distiller, self).compile(optimizer=optimizer, metrics=metrics) # compile the model with the optimizer and metrics\n",
    "        self.student_loss_fn = student_loss_fn\n",
    "        self.distillation_loss_fn = distillation_loss_fn\n",
    "        self.alpha = alpha\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def train_step(self, data): # this function is used to train the model using the distillation loss\n",
    "        x, y = data\n",
    "        teacher_predictions = self.teacher(x, training=False)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            student_predictions = self.student(x, training=True)\n",
    "            student_loss = self.student_loss_fn(y, student_predictions)\n",
    "            distillation_loss = self.distillation_loss_fn(\n",
    "                tf.nn.softmax(teacher_predictions / self.temperature, axis=1),\n",
    "                tf.nn.softmax(student_predictions / self.temperature, axis=1),\n",
    "            )\n",
    "            loss = self.alpha * student_loss + (1 - self.alpha) * distillation_loss # calculate the loss using the distillation loss\n",
    "\n",
    "        gradients = tape.gradient(loss, self.student.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.student.trainable_variables))\n",
    "        self.compiled_metrics.update_state(y, student_predictions)\n",
    "        results = {m.name: m.result() for m in self.metrics}\n",
    "        results.update({\"student_loss\": student_loss, \"distillation_loss\": distillation_loss}) # update the results with the student loss and distillation loss\n",
    "        return results\n",
    "\n",
    "    #Test 2\n",
    "    def test_step(self, data): # this function is used to test the model using the distillation loss\n",
    "        x, y = data\n",
    "        student_predictions = self(x, training=False)\n",
    "        student_loss = self.student_loss_fn(y, student_predictions)\n",
    "\n",
    "        # Update metrics\n",
    "        self.compiled_metrics.update_state(y, student_predictions)\n",
    "\n",
    "        # Collect metrics results\n",
    "        results = {m.name: m.result() for m in self.metrics}\n",
    "        results['loss'] = student_loss\n",
    "        return results\n",
    "\n",
    "    def call(self, inputs, training=False): # this function is used to call the model\n",
    "        if training:\n",
    "            return self.student(inputs, training=True)\n",
    "        else:\n",
    "            return self.student(inputs, training=False)\n",
    "\n",
    "# Create the distiller instance again and compile it\n",
    "distiller = Distiller(student=student, teacher=teacher)\n",
    "distiller.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    metrics=[tf.keras.metrics.BinaryAccuracy()],\n",
    "    student_loss_fn=tf.keras.losses.BinaryCrossentropy(),\n",
    "    distillation_loss_fn=tf.keras.losses.KLDivergence(),\n",
    "    alpha=0.1,\n",
    "    temperature=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard initialization\n",
    "from datetime import datetime\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "path_to_logs = \"C:/Users/ziadt/Desktop/Projects/MSc Project implimintation/Model Training/CustomeNEt/training supporting material/logs/fit/\"\n",
    "\n",
    "# Create a TensorBoard callback\n",
    "log_dir = path_to_logs + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the distiller\n",
    "history = distiller.fit(train_ds, epochs=1, batch_size=32, validation_data=val_ds, callbacks=[tensorboard_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy perfomance on the test set\n",
    "distiller.evaluate(test_ds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the Custom model without Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "student.save('C:/Users/ziadt/Desktop/Projects/MSc Project implimintation/Model Training/CustomeNEt/training supporting material/student_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "from tensorflow.keras.models import load_model\n",
    "model = load_model('C:/Users/ziadt/Desktop/Projects/MSc Project implimintation/Model Training/CustomeNEt/training supporting material/student_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post Training Interger Quantization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full integer quantization\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(distiller.student)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model to disk\n",
    "open('QCustom_D1_V1.tflite', 'wb').write(tflite_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference speed of the QCustom model on a single slot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Load the TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output tensors.\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "\n",
    "# test data\n",
    "n = 0\n",
    "image_path = test_images[n]\n",
    "full_image_path = os.path.join(base_path, image_path)\n",
    "\n",
    "image = cv2.imread(full_image_path)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "image = cv2.resize(image, (150, 150))\n",
    "image = np.expand_dims(image, axis=0)\n",
    "image = image / 255.0\n",
    "image = tf.convert_to_tensor(image, dtype=tf.float32)\n",
    "\n",
    "\n",
    "# Test the model on random input data.\n",
    "times = []\n",
    "for i in range(100):\n",
    "    start_time = time.time()\n",
    "    interpreter.set_tensor(input_details[0]['index'], image)\n",
    "    interpreter.invoke()\n",
    "    end_time = time.time()\n",
    "    times.append(end_time - start_time)\n",
    "\n",
    "print('Average inference time for 100 images with quantized model:', np.mean(times))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newvm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
